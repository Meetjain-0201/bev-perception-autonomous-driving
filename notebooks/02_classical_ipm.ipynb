{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üó∫Ô∏è Classical IPM (Inverse Perspective Mapping)\n",
    "## Geometric Transformation: Camera ‚Üí BEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # For saving, not displaying\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.models.ipm import InversePerspectiveMapping\n",
    "from src.data.dataset import NuScenesMultiViewDataset\n",
    "\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading nuScenes v1.0-mini (train split)...\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.817 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n",
      "Loaded 323 samples for train\n",
      "Loaded sample with 6 cameras\n"
     ]
    }
   ],
   "source": [
    "dataset = NuScenesMultiViewDataset(\n",
    "    data_root='../data/nuscenes',\n",
    "    version='v1.0-mini',\n",
    "    split='train',\n",
    "    image_size=(224, 400)\n",
    ")\n",
    "\n",
    "sample = dataset[0]\n",
    "print(f\"Loaded sample with {len(dataset.cameras)} cameras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create IPM Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEV grid: 200x200\n",
      "Coverage: 50m √ó 45m\n"
     ]
    }
   ],
   "source": [
    "ipm = InversePerspectiveMapping(\n",
    "    image_size=(224, 400),\n",
    "    bev_size=(200, 200),\n",
    "    bev_range=(-25, 25, 5, 50)  # 25m left/right, 5-50m forward\n",
    ")\n",
    "\n",
    "print(f\"BEV grid: {ipm.bev_h}x{ipm.bev_w}\")\n",
    "print(f\"Coverage: {ipm.x_max-ipm.x_min}m √ó {ipm.y_max-ipm.y_min}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transform Front Camera to BEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: (224, 400, 3)\n",
      "Output BEV: (200, 200, 3)\n",
      "Coverage: 66.2%\n"
     ]
    }
   ],
   "source": [
    "# Get front camera (index 0)\n",
    "front_img = sample['images'][0].numpy().transpose(1, 2, 0)\n",
    "front_img = (front_img * 255).astype(np.uint8)\n",
    "front_K = sample['intrinsics'][0].numpy()\n",
    "front_ext = sample['extrinsics'][0].numpy()\n",
    "\n",
    "# Apply IPM\n",
    "bev_front = ipm.create_bev_from_camera(front_img, front_K, front_ext)\n",
    "\n",
    "print(f\"Input: {front_img.shape}\")\n",
    "print(f\"Output BEV: {bev_front.shape}\")\n",
    "print(f\"Coverage: {100*np.count_nonzero(bev_front)/(200*200*3):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: results/images/ipm_front_camera.png\n"
     ]
    }
   ],
   "source": [
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "axes[0].imshow(front_img)\n",
    "axes[0].set_title('CAM_FRONT (Perspective View)', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(bev_front, origin='lower')\n",
    "axes[1].set_title('BEV (IPM Transformation)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('X: Left (-25m) ‚Üê ‚Üí Right (+25m)', fontsize=11)\n",
    "axes[1].set_ylabel('Y: Close (5m) ‚Üë Far (50m)', fontsize=11)\n",
    "axes[1].grid(True, color='yellow', alpha=0.3, linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/images/ipm_front_camera.png', dpi=150, bbox_inches='tight')\n",
    "print(\"‚úÖ Saved: results/images/ipm_front_camera.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply to All 6 Cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: results/images/ipm_all_cameras.png\n"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "for idx in range(6):\n",
    "    img = sample['images'][idx].numpy().transpose(1, 2, 0)\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    K = sample['intrinsics'][idx].numpy()\n",
    "    ext = sample['extrinsics'][idx].numpy()\n",
    "    \n",
    "    bev = ipm.create_bev_from_camera(img, K, ext)\n",
    "    \n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    ax.imshow(bev, origin='lower')\n",
    "    ax.set_title(f\"{dataset.cameras[idx]} ‚Üí BEV\", fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('X (m)', fontsize=9)\n",
    "    ax.set_ylabel('Y (m)', fontsize=9)\n",
    "    ax.grid(True, color='white', alpha=0.2, linewidth=0.5)\n",
    "\n",
    "plt.suptitle('IPM Applied to All 6 Cameras', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/images/ipm_all_cameras.png', dpi=150, bbox_inches='tight')\n",
    "print(\"‚úÖ Saved: results/images/ipm_all_cameras.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. IPM Limitations Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: results/images/ipm_limitations.png\n"
     ]
    }
   ],
   "source": [
    "# Zoom into BEV to see artifacts\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Full BEV\n",
    "axes[0].imshow(bev_front, origin='lower')\n",
    "axes[0].set_title('Full BEV', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, color='yellow', alpha=0.3)\n",
    "\n",
    "# Zoomed region (center, where cars might be)\n",
    "center_crop = bev_front[50:150, 50:150]\n",
    "axes[1].imshow(center_crop, origin='lower')\n",
    "axes[1].set_title('Zoomed: Notice Stretched/Distorted Objects', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, color='yellow', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/images/ipm_limitations.png', dpi=150, bbox_inches='tight')\n",
    "print(\"‚úÖ Saved: results/images/ipm_limitations.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Key Observations\n",
    "\n",
    "**What IPM Does Well:**\n",
    "- ‚úÖ Transforms road surface to top-down view\n",
    "- ‚úÖ Lane markings visible (if present)\n",
    "- ‚úÖ Fast (pure geometry, no neural network)\n",
    "- ‚úÖ Interpretable (know exactly what it's doing)\n",
    "\n",
    "**What IPM Fails At:**\n",
    "- ‚ùå **3D objects get distorted** (cars, pedestrians stretched)\n",
    "- ‚ùå **Assumes flat ground** (fails on hills, ramps)\n",
    "- ‚ùå **No depth understanding** (everything projected to Z=0)\n",
    "- ‚ùå **Occlusions not handled**\n",
    "\n",
    "**Why This Happens:**\n",
    "```\n",
    "IPM assumes:  Everything is on the ground (Z=0)\n",
    "Reality:      Cars have height! (Z ‚â† 0)\n",
    "\n",
    "Result:       Cars get 'smeared' across the ground plane\n",
    "```\n",
    "\n",
    "**Solution:** Neural methods that learn depth!\n",
    "\n",
    "## üéØ Next: Lift-Splat-Shoot (LSS)\n",
    "\n",
    "LSS will:\n",
    "1. **Lift:** Predict depth for each pixel ‚Üí make 3D\n",
    "2. **Splat:** Scatter into 3D voxel grid\n",
    "3. **Shoot:** Project to BEV (now with 3D understanding!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BEV Perception",
   "language": "python",
   "name": "bev-perception"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

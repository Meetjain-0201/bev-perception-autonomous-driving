{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üó∫Ô∏è Classical IPM (Inverse Perspective Mapping)\n",
    "## Transform Camera Images to Bird's Eye View\n",
    "\n",
    "**Goal:** Understand geometric transformation from perspective to BEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from src.models.ipm import InversePerspectiveMapping\n",
    "from src.data.dataset import NuScenesMultiViewDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = NuScenesMultiViewDataset(\n",
    "    data_root='../data/nuscenes',\n",
    "    version='v1.0-mini',\n",
    "    split='train',\n",
    "    image_size=(224, 400)\n",
    ")\n",
    "\n",
    "# Get one sample\n",
    "sample = dataset[0]\n",
    "print(f\"Loaded sample with {sample['images'].shape[0]} cameras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apply IPM to Front Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create IPM transformer\n",
    "ipm = InversePerspectiveMapping(\n",
    "    image_size=(224, 400),\n",
    "    bev_size=(200, 200),\n",
    "    bev_range=(-25, 25, 0, 50)  # 25m left/right, 50m forward\n",
    ")\n",
    "\n",
    "# Get front camera (index 0)\n",
    "front_image = sample['images'][0].numpy().transpose(1, 2, 0)  # (H, W, 3)\n",
    "front_image = (front_image * 255).astype(np.uint8)\n",
    "front_K = sample['intrinsics'][0].numpy()\n",
    "front_extrinsics = sample['extrinsics'][0].numpy()\n",
    "\n",
    "# Transform to BEV\n",
    "bev_image = ipm.transform_image_to_bev(\n",
    "    front_image,\n",
    "    front_K,\n",
    "    front_extrinsics\n",
    ")\n",
    "\n",
    "print(f\"Input image: {front_image.shape}\")\n",
    "print(f\"Output BEV: {bev_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize: Camera vs BEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Camera view\n",
    "axes[0].imshow(front_image)\n",
    "axes[0].set_title('Camera View (Perspective)', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# BEV view\n",
    "axes[1].imshow(bev_image, origin='lower')\n",
    "axes[1].set_title('Bird\\'s Eye View (IPM)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('X (lateral, meters)', fontsize=12)\n",
    "axes[1].set_ylabel('Y (forward, meters)', fontsize=12)\n",
    "\n",
    "# Add grid\n",
    "axes[1].grid(True, alpha=0.3, color='white', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/images/ipm_transformation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Saved to results/images/ipm_transformation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Camera BEV Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform all 6 cameras to BEV\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "cameras = dataset.cameras\n",
    "\n",
    "for idx in range(6):\n",
    "    # Get camera data\n",
    "    img = sample['images'][idx].numpy().transpose(1, 2, 0)\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    K = sample['intrinsics'][idx].numpy()\n",
    "    extrinsics = sample['extrinsics'][idx].numpy()\n",
    "    \n",
    "    # Transform to BEV\n",
    "    bev = ipm.transform_image_to_bev(img, K, extrinsics)\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    ax.imshow(bev, origin='lower')\n",
    "    ax.set_title(f\"{cameras[idx]} ‚Üí BEV\", fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('IPM Applied to All 6 Cameras', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/images/ipm_all_cameras.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Saved to results/images/ipm_all_cameras.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze IPM Limitations\n",
    "\n",
    "**What IPM does well:**\n",
    "- ‚úÖ Road surface transformation\n",
    "- ‚úÖ Lane markings\n",
    "- ‚úÖ Fast (no neural network)\n",
    "- ‚úÖ Interpretable (pure geometry)\n",
    "\n",
    "**What IPM fails at:**\n",
    "- ‚ùå 3D objects (cars, pedestrians)\n",
    "- ‚ùå Elevated structures (bridges, signs)\n",
    "- ‚ùå Non-flat terrain (hills, ramps)\n",
    "- ‚ùå Occlusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom into BEV to see distortions on cars\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(bev_image, origin='lower')\n",
    "plt.title('IPM Result - Notice Cars are Distorted!', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('X (meters)')\n",
    "plt.ylabel('Y (meters)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('../results/images/ipm_limitations.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Observation:\")\n",
    "print(\"   Cars appear 'stretched' and distorted in BEV\")\n",
    "print(\"   This is because IPM assumes everything is on the ground (Z=0)\")\n",
    "print(\"   But cars have height! They stick up above the ground.\")\n",
    "print(\"\\n   This is why we need NEURAL methods (LSS) next!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Phase 2 Complete!\n",
    "\n",
    "**What we learned:**\n",
    "- ‚úÖ IPM uses homography (3√ó3 matrix) for transformation\n",
    "- ‚úÖ Works by assuming flat ground plane (Z=0)\n",
    "- ‚úÖ Fast and interpretable\n",
    "- ‚úÖ But fails for 3D objects\n",
    "\n",
    "**Next: Implement LSS (Neural BEV Transformation)**\n",
    "- Predict depth for each pixel\n",
    "- Lift to 3D, splat to voxels, shoot to BEV\n",
    "- Handle 3D objects correctly!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BEV Perception",
   "language": "python",
   "name": "bev-perception"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
